---
title: "Text Analysis with R"
author: "Sean"
date: "11/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setting up and Getting Started with R

Now that you have R and RStudio installed, let's do a few things to make sure that everything is working properly.

First, find the "Session" menu tab at the top of the window (5 menu tabs to the right of "File."). Click "Session" and then navigate through "Set Working Directory" to "Choose Directory." Follow the file chooser dialog and choose/make a directory that you want to work with for the workshop today. When you have finished this step, look in the console pane of RStudio. You should see something like this:

> setwd("~/folderYouChose")

Congratulations, you've just run your first piece of R code! The setwd() command that R produced is a built-in function that sets your working directory to a folder of your choice. This can be an important step for when you are writing to and reading from files for a project you may be working on.

Now let's do one last check to ensure that R is working properly. Go ahead and run the chunk of code below:

```{r math}

1+1 # r can do the math
2*2 # r can also do the comment
459689230409/9034953849723
2*pi # r knows some digits of pi
x <- 2 # r can assign the variable
y <- 27 # r can assign the other variable
x^y # r can raise to a power

```

If the console prints 5 results, then you are good to go with R and RStudio.

## Working with Text as Data

When we humans read text, we do not necessarily consider individual words. Instead, we consider the larger work (a novel, a play, a news article, a facebook status update) and perhaps groups of words that together inform our understanding of the text we are reading and give context to the individual words themselves. This is a complex *learning* process!

In computationally-assisted text analysis, the process is a little different. When working with text, an interested analyst will perform a couple of steps to preprocess text data for computation, often making decisions about how to impose some meaningful structure on the text. For instance, when working with a novel (or many novels), one might choose to divide text into chunks based on chapter divisions or an arbitrary number of tokens (words or sentences).

Such a dilemma, that becomes particularly visible when working with text, bears implications for how we conceive of data and insights. Johanna Drucker explains this phenomenon expertly:

>"Differences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is “taken” actively while data is assumed to be a “given” able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.”

As we start analyzing text, keep in mind how the decisions we make to tokenize and categorize *aid* our computation and *influence* our insights.

### Text Analysis with Base R

Before we get going using some helpful tokenization packages, let's take a look at how we might tokenize text to make observations about word counts with basic R syntax - to see how easy it will be later but also to think about how some of the packages we will be using may be working behind the scenes.

```{r base analysis, message = F, warning = F}

# load relevant libraries
library(gutenbergr)
library(dplyr)

# load moby dick text from gutenberg
moby_txt <- gutenberg_download(2701)

# just grab the text of the novel
moby.txt <- moby_txt$text

# find index of where novel actually starts
start.v <- which(moby.txt == "  CHAPTER 1")

# find index of where novel actually ends
end.v <- which(moby.txt == "after her missing children, only found another orphan.")

# index moby.txt character vector by start and end indices 
novel.lines.v <- moby.txt[start.v:end.v]

# paste all lines of text into one string of text
novel.v <- paste(novel.lines.v, collapse=" ")

# put all text "to lower case"
novel.lower.v <- tolower(novel.v)

# split giant text string at every non-word character (regex)
novel.words <- strsplit(novel.lower.v, "\\W")

# strsplit makes a list object, we will want to work back with a vector
moby.words.v <- unlist(novel.words)

# index any word characters that are not blanks
not.blanks.v <- which(moby.words.v != "")

# only keep those words that are not blanks
moby.words.v <- moby.words.v[not.blanks.v]

# check our output of first ten words of novel
moby.words.v[1:10]

# tabulate raw word counts of all words in our moby dick words vector
moby.freqs.t <- table(moby.words.v)

# sort all counts by decreasing value
sorted.moby.freqs.t <- sort(moby.freqs.t, decreasing=TRUE)

# map to dataframe for ease of use
sorted.moby.freqs.df <- as_tibble(sorted.moby.freqs.t)

# pipe top ten words to a standard r plot
sorted.moby.freqs.df$n[1:10] %>%
plot(main="Plot of Top Ten Frequent Moby Dick Words", 
     xlab="Words", ylab="Frequencies", xaxt="n")
axis(1,1:10, labels=sorted.moby.freqs.df$moby.words.v[1:10])

```

### Working with Packages

Before we get started, let's load some necessary libraries. R, like many other open source languages, has a vibrant community of developers and analysts who distribute packages that facilitate specific tasks, like preprocessing text data.

Uncomment the install statements and run the block below to install necessary packages and then to load them into your session.

```{r loading libraries, message=FALSE, warning=FALSE}

#install.packages('tidyverse')
#install.packages('tidytext')
#install.packages('SnowballC')
#install.packages('gutenbergr')
#install.packages('gmodels')
#install.packages('scales')

library(tidyverse)
library(tidytext)
library(SnowballC)
library(gutenbergr)
library(gmodels)
library(scales)

```

To make sure some of the more vital functions of the packages have been loaded properly, run the following code block:

```{r testing packages}

JD_capta <- c("Differences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is taken actively while data is assumed to be a given able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.")

JD_df <- tibble(JD_capta)

JD_df %>% 
  unnest_tokens(output = word, input = JD_capta, token = "words")

JD_df %>%
  unnest_tokens(output = bigram, input = JD_capta, token = "ngrams", n=2)

# What do you notice about each result that prints? In each case, what is the "token" for analysis? Why might we choose each type of tokenization?



# Adjust the above code to tokenize the text into different ngrams (trigrams perhaps)




```

```{r help, message = F}

# Now, try to tokenize the text by sentences (hint: use the file and help pane to find out more about the unnest_tokens; run the line below to quickly access help)
?unnest_tokens




```

Now that we've gotten a handle on tokenizing text with some packages developed for R, let's play with some more interesting text data.

We'll be using Jane Austen's *Pride and Prejudice*. R has a convenient "janeaustenr" package that has the text of most of Jane Austen's works. We'll go ahead and load *Pride and Prejudice* into a dataframe (tibble) and see what kinds of observations we can make.

We'll start by tokenizing the text of Jane Austen using regex. Run the block of code and observe what happens (uncomment and install 'janeaustenr' if you have not used this package before).

```{r working with Jane, message = FALSE, warning = FALSE}

#install.packages('janeaustenr')
library(janeaustenr)
p_and_p <- tibble(txt = prideprejudice)

segments <- p_and_p %>%
  unnest_tokens(segment, txt, token = "regex", pattern = "-")

head(segments) # outputs the first 6 segments based on our regex pattern

```

This split, while interesting, doesn't seem entirely useful. Novels like *Pride and Prejudice*, though, are typically broken up by chapter. Below, use the regex tokenization features of unnest_tokens to break *Pride and Prejudice* up by chapter and print the first 6 results to your console.

```{r breaking by chapter}

chapters <- p_and_p %>%
  unnest_tokens(chapter, txt, token = "regex", pattern = "")

# output first 6 results below



```

## Comparing Data and Making Observations

```{r comparing and observing}

# generating raw count of word tokens in the Johanna Drucker quotation
jd_count <- JD_df %>%
  unnest_tokens(output = word, input = JD_capta) %>%
  count(word, sort = TRUE)

# generate a raw count of word tokens in Pride and Prejudice



```

```{r answer1, echo = F, results = 'hide'}

# Raw word count P&P
pride_count <- p_and_p %>%
  unnest_tokens(output = word, input = txt) %>%
  count(word, sort = TRUE)

```

The word counts we've generated represent *raw frequencies*, meaning that they just tell us how many of each word type are found in each text analyzed. *Relative frequencies* give us similar information, but tell us specifically what percentage of the text in question each word token comprises.

Recognizing this difference, calculate the relative frequencies for each word token and append that information to your current data frames (for the Johanna Drucker quotation word counts and the *Pride and Prejudice* counts). R, like most programming langauges has syntax unique to itself. To aid your efforts, we've provided an example of how you might add a new column like a relative freqruency one to an existing data frame.

```{r relative frequencies}

# example of adding a column to a data frame
pride_count$new_column <- 1:nrow(pride_count)

# and to get rid of this totally unnecessary column we could do something like this:
pride_count <- pride_count[-3]

# Add the relative frequencies as a separate column to your data frames



# As a bonus, remove the raw frequency column ("n"). Remember that R, unlike most other languages, begins indexing at 1, not 0.




```

```{r answer2, echo = F, results = 'hide'}

pride_count$rel_freqs <- 100*pride_count$n/sum(pride_count$n)
jd_count$rel_freqs <- 100*jd_count$n/sum(jd_count$n)

```

Now, let's plot some of these relative frequencies to gain a sense of what words each text seems to use, well, frequently!

```{r plotting frequencies, message = F, warning = F}

# load a plotting library
library(ggplot2)

# plot each 
pride_count %>%
  head(10) %>%
  mutate(word = reorder(word,rel_freqs)) %>% #reorders word frame by n
  ggplot(aes(word,rel_freqs),size = 7) +
  geom_col() +
  xlab(NULL) +
  theme_minimal()+
  coord_flip()

jd_count %>%
  head(10) %>%
  mutate(word = reorder(word,rel_freqs)) %>% #reorders word frame by n
  ggplot(aes(word,rel_freqs),size = 7) +
  geom_col() +
  xlab(NULL) +
  theme_minimal()+
  coord_flip()

# looking at these two plots, what are some observations you have about top frequent words used?




```

### Removing Stop Words and Stemming

It's cool to compare top frequent words from our different textual data sets, but we get a lot of words that may or may not convey much of what our texts are about ("the", "of", "and"). We also may get different variations of the same words:

```{r different variations example}

pride_count[grep("choos\\w+", pride_count$word),]

```

While there may be some semantic difference between these variations of "choose," based on our research question, we could decide that we want to find a way to group all of these versions of "choose" together under one set of counts. To do both of these things, removing words that may or may not contribute much to meaning and to collapse multiple variations of words under one token, we'll get rid of stopwords from our sets and stem our word tokens.

```{r stop words and stemming, message = F}

# tokenizing text and removing stop words and stemming
jd_count_ns_stem <- JD_df %>%
  unnest_tokens(output = word, input = JD_capta) %>%
  anti_join(stop_words)

jd_count_ns_stem$word <- wordStem(jd_count_ns_stem$word)
jd_count_ns_stem <- jd_count_ns_stem %>%
  count(word, sort = TRUE)

p_count_ns_stem <- p_and_p %>%
  unnest_tokens(output = word, input = txt) %>%
  anti_join(stop_words)

p_count_ns_stem$word <- wordStem(p_count_ns_stem$word)
p_count_ns_stem <- p_count_ns_stem %>%
  count(word, sort = TRUE)

# now find find all instances of "choos" in this new dataframe and compare with what we found earlier.




```

Finally, let's look at how removing stop words and stemming changes our relative frequency plots

```{r changed rel freqs}

# calculate relative frequencies as we did earlier



# plot each 



# looking at these two plots, what are some observations you have about top frequent words used?




```


