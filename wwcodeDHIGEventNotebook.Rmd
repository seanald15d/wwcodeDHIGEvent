---
title: "Text Analysis with R"
author: "Sean"
date: "11/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setting up and Getting Started with R

Now that you have R and RStudio installed, let's do a few things to make sure that everything is working properly.

Go ahead and run the chunk of code below:

```{r math}
# if this block does not run, execute the line below
#install.packages('knitr')

1+1 # r can do the math
2*2 # r can also do the comment
459689230409/9034953849723
2*pi # r knows some digits of pi
x <- 2 # r can assign the variable
y <- 27 # r can assign the other variable
x^y # r can raise to a power

```

If the console prints 5 results, then you are good to go with R and RStudio.

## Working with Text as Data

When we humans read text, we do not necessarily consider individual words. Instead, we consider the larger work (a novel, a play, a news article, a facebook status update) and perhaps groups of words that together inform our understanding of the text we are reading and give context to the individual words themselves. This is a complex *learning* process!

In computationally-assisted text analysis, the process is a little different. When working with text, an interested analyst will perform a couple of steps to preprocess text data for computation, often making decisions about how to impose some meaningful structure on the text. For instance, when working with a novel (or many novels), one might choose to divide text into chunks based on chapter divisions or an arbitrary number of tokens (words or sentences).

Such a dilemma, that becomes particularly visible when working with text, bears implications for how we conceive of data and insights. Johanna Drucker explains this phenomenon expertly:

>"Differences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is “taken” actively while data is assumed to be a “given” able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.”

As we start analyzing text, keep in mind how the decisions we make to tokenize and categorize *aid* our computation and *influence* our insights.

### Text Analysis with Base R

Before we get going using some helpful tokenization packages, let's take a look at how we might tokenize text to make observations about word counts with basic R syntax - to see how easy it will be later but also to think about how some of the packages we will be using may be working behind the scenes.

```{r base analysis, message = F, warning = F}

# load relevant libraries
#install.packages('gutenbergr') # uncomment and run these first two lines if packages do not load
#install.packages('dplyr')
library(gutenbergr)
library(dplyr)

# load moby dick text from gutenberg
moby_txt <- gutenberg_download(2701)

# just grab the text of the novel
moby.txt <- moby_txt$text

# find index of where novel actually starts
start.v <- which(moby.txt == "  CHAPTER 1")

# find index of where novel actually ends
end.v <- which(moby.txt == "after her missing children, only found another orphan.")

# index moby.txt character vector by start and end indices 
novel.lines.v <- moby.txt[start.v:end.v]

# paste all lines of text into one string of text
novel.v <- paste(novel.lines.v, collapse=" ")

# put all text "to lower case"
novel.lower.v <- tolower(novel.v)

# split giant text string at every non-word character (regex)
novel.words <- strsplit(novel.lower.v, "\\W")

# strsplit makes a list object, we will want to work back with a vector
moby.words.v <- unlist(novel.words)

# index any word characters that are not blanks
not.blanks.v <- which(moby.words.v != "")

# only keep those words that are not blanks
moby.words.v <- moby.words.v[not.blanks.v]

# check our output of first ten words of novel
moby.words.v[1:10]

# tabulate raw word counts of all words in our moby dick words vector
moby.freqs.t <- table(moby.words.v)

# sort all counts by decreasing value
sorted.moby.freqs.t <- sort(moby.freqs.t, decreasing=TRUE)

# map to dataframe for ease of use
sorted.moby.freqs.df <- as_tibble(sorted.moby.freqs.t)

# pipe top ten words to a standard r plot
sorted.moby.freqs.df$n[1:10] %>%
plot(main="Plot of Top Ten Frequent Moby Dick Words", 
     xlab="Words", ylab="Frequencies", xaxt="n")
axis(1,1:10, labels=sorted.moby.freqs.df$moby.words.v[1:10])

```

### Working with Packages

Before we get started, let's load some necessary libraries. R, like many other open source languages, has a vibrant community of developers and analysts who distribute packages that facilitate specific tasks, like preprocessing text data.

Uncomment the install statements and run the block below to install necessary packages and then to load them into your session.

```{r loading libraries, message=FALSE, warning=FALSE}

#install.packages('tidyverse')
#install.packages('tidytext')
#install.packages('SnowballC')
#install.packages('gmodels')

library(tidyverse)
library(tidytext)
library(SnowballC)
library(gmodels)

```

To make sure some of the more vital functions of the packages have been loaded properly, run the following code block:

```{r testing packages}

JD_capta <- c("Differences in the etymological roots of the terms data and capta make the distinction between constructivist and realist approaches clear. Capta is taken actively while data is assumed to be a given able to be recorded and observed. From this distinction, a world of differences arises. Humanistic inquiry acknowledges the situated, partial, and constitutive character of knowledge production, the recognition that knowledge is constructed, taken, not simply given as a natural representation of pre-existing fact.")

JD_df <- tibble(JD_capta)

JD_df %>% 
  unnest_tokens(output = word, input = JD_capta, token = "words")

JD_df %>%
  unnest_tokens(output = bigram, input = JD_capta, token = "ngrams", n=2)

# What do you notice about each result that prints? In each case, what is the "token" for analysis? Why might we choose each type of tokenization?



# Adjust the above code to tokenize the text into different ngrams (trigrams perhaps)




```

```{r help, message = F}

# Now, try to tokenize the text by sentences (hint: use the file and help pane to find out more about the unnest_tokens; run the line below to quickly access help)
?unnest_tokens




```

Now that we've gotten a handle on tokenizing text with some packages developed for R, let's play with some more interesting text data.

We'll be using Jane Austen's *Pride and Prejudice*. R has a convenient "janeaustenr" package that has the text of most of Jane Austen's works. We'll go ahead and load *Pride and Prejudice* into a dataframe (tibble) and see what kinds of observations we can make.

We'll start by tokenizing the text of Jane Austen using regex. Run the block of code and observe what happens (uncomment and install 'janeaustenr' if you have not used this package before).

```{r working with Jane, message = FALSE, warning = FALSE}

#install.packages('janeaustenr')
library(janeaustenr)
p_and_p <- tibble(txt = prideprejudice)

segments <- p_and_p %>%
  unnest_tokens(segment, txt, token = "regex", pattern = "-")

head(segments) # outputs the first 6 segments based on our regex pattern

```

This split, while interesting, doesn't seem entirely useful. Novels like *Pride and Prejudice*, though, are typically broken up by chapter. Below, use the regex tokenization features of unnest_tokens to break *Pride and Prejudice* up by chapter and print the first 6 results to your console.

```{r breaking by chapter}

chapters <- p_and_p %>%
  unnest_tokens(chapter, txt, token = "regex", pattern = "")

# output first 6 results below



```

## Comparing Data and Making Observations

```{r comparing and observing}

# generating raw count of word tokens in the Johanna Drucker quotation
jd_count <- JD_df %>%
  unnest_tokens(output = word, input = JD_capta) %>%
  count(word, sort = TRUE)

# generate a raw count of word tokens in Pride and Prejudice



```

```{r answer1, echo = F, results = 'hide'}

# Raw word count P&P
pride_count <- p_and_p %>%
  unnest_tokens(output = word, input = txt) %>%
  count(word, sort = TRUE)

```

The word counts we've generated represent *raw frequencies*, meaning that they just tell us how many of each word type are found in each text analyzed. *Relative frequencies* give us similar information, but tell us specifically what percentage of the text in question each word token comprises.

Recognizing this difference, calculate the relative frequencies for each word token and append that information to your current data frames (for the Johanna Drucker quotation word counts and the *Pride and Prejudice* counts). R, like most programming langauges has syntax unique to itself. To aid your efforts, we've provided an example of how you might add a new column like a relative freqruency one to an existing data frame.

```{r relative frequencies}

# example of adding a column to a data frame
pride_count$new_column <- 1:nrow(pride_count)

# and to get rid of this totally unnecessary column we could do something like this:
pride_count <- pride_count[-3]

# Add the relative frequencies as a separate column to your data frames



# As a bonus, remove the raw frequency column ("n"). Remember that R, unlike most other languages, begins indexing at 1, not 0.




```

```{r answer2, echo = F, results = 'hide'}

pride_count$rel_freqs <- 100*pride_count$n/sum(pride_count$n)
jd_count$rel_freqs <- 100*jd_count$n/sum(jd_count$n)

```

Now, let's plot some of these relative frequencies to gain a sense of what words each text seems to use, well, frequently!

```{r plotting frequencies, message = F, warning = F}

# load a plotting library
library(ggplot2)

# plot each 
pride_count %>%
  head(10) %>%
  mutate(word = reorder(word,rel_freqs)) %>% #reorders word frame by n
  ggplot(aes(word,rel_freqs),size = 7) +
  geom_col() +
  xlab(NULL) +
  theme_minimal()+
  coord_flip()

jd_count %>%
  head(10) %>%
  mutate(word = reorder(word,rel_freqs)) %>% #reorders word frame by n
  ggplot(aes(word,rel_freqs),size = 7) +
  geom_col() +
  xlab(NULL) +
  theme_minimal()+
  coord_flip()

# looking at these two plots, what are some observations you have about top frequent words used?




```

### Removing Stop Words and Stemming

It's cool to compare top frequent words from our different textual data sets, but we get a lot of words that may or may not convey much of what our texts are about ("the", "of", "and"). We also may get different variations of the same words:

```{r different variations example}

pride_count[grep("choos\\w+", pride_count$word),]

```

While there may be some semantic difference between these variations of "choose," based on our research question, we could decide that we want to find a way to group all of these versions of "choose" together under one set of counts. To do both of these things, removing words that may or may not contribute much to meaning and to collapse multiple variations of words under one token, we'll get rid of stopwords from our sets and stem our word tokens.

```{r stop words and stemming, message = F}

# tokenizing text and removing stop words and stemming
jd_count_ns_stem <- JD_df %>%
  unnest_tokens(output = word, input = JD_capta) %>%
  anti_join(stop_words)

jd_count_ns_stem$word <- wordStem(jd_count_ns_stem$word)
jd_count_ns_stem <- jd_count_ns_stem %>%
  count(word, sort = TRUE)

p_count_ns_stem <- p_and_p %>%
  unnest_tokens(output = word, input = txt) %>%
  anti_join(stop_words)

p_count_ns_stem$word <- wordStem(p_count_ns_stem$word)
p_count_ns_stem <- p_count_ns_stem %>%
  count(word, sort = TRUE)

# now find find all instances of "choose" in this new dataframe and compare with what we found earlier.




```

Finally, let's look at how removing stop words and stemming changes our relative frequency plots

```{r changed rel freqs}

# calculate relative frequencies as we did earlier



# plot each



# looking at these two plots, what are some observations you have about top frequent words used?




```

So plotting top frequent words is nice and can give us an idea of what an author's focus may be, but there are so many more things we can do with text data. An important thing to note, though, is that most text mining projects begin with preprocessing: making decisions about how to tokenize text, what words and/or characters to exclude (if any), and about how to account for words and features surrounding word use.

## An Example Machine Learning Application

Let's say we're at a fancy party, and someone has the audacity to spark a debate concerning the superiority of dog or cat owners. Everyone scoffs in disbelief, but they all secretly want to weigh in. The conversation gets so heated that the nefarious ne'er do well who started it finally says, "Well, I personally like cats more, but I know I cannot reach the dog people in the room because they just seem to speak a different language." 

How could Mr. Cat-Arguer say such a thing? How could we verify his salacious surmisal?

First, let's break down the question:

1. Do cat and dog people speak different langauges?

How could we measure such a phenomenon? One way is to gather a corpus of texts related to dog and cat fandom and to do work similar to what we did earlier.

For this exercise, we'll be using Reddit and R's RedditExtractoR package. Run the lines below to install this package and load it into your session:

```{r install RedditExtractoR, warning = F, message = F}

#install.packages('RedditExtractoR')
library(RedditExtractoR)

```

Now, let's pull some data.

```{r pulling data, message = F, warning = F, results = 'hide'}

# searching reddit links for cats and dogs
cat_links <- reddit_urls(
  search_terms = "cats",
  page_threshold = 1
)

dog_links <- reddit_urls(
  search_terms = "dogs",
  page_threshold = 1
)

# look at each data frame. The subreddit category tells you on what channel
# each post (and its comments) lives. What subreddit and post title do you
# think will lead to a nice comparison? For now (and because things will
# change by the workshop), I've chosen some that may not apply later...

cat_links <- subset(cat_links, subreddit == 'cats')

dog_links <- subset(dog_links, subreddit == 'dogs')

# write function to apply to links
link_getter <- function(x){
  thread <- reddit_content(x[5])
  return(thread)
}

# call function applied to each row of df and bind resulting list of dfs
# into single df
cat_thread <- apply(cat_links, MARGIN = 1, link_getter)
cat_thread.df <- bind_rows(cat_thread, .id = "column_label")

# repeat for dogs
dog_thread <- apply(dog_links, MARGIN = 1, link_getter)
dog_thread.df <- bind_rows(dog_thread, .id = "column_label")

# let's randomize our data and extract the same number of observations from each df
set.seed(123)
r_cat_thread.df <- cat_thread.df[sample(1:nrow(cat_thread.df)),]
r_dog_thread.df <- dog_thread.df[sample(1:nrow(dog_thread.df)),]

trimmed_r_cat_thread.df <- r_cat_thread.df[1:2000,]
trimmed_r_dog_thread.df <- r_dog_thread.df[1:2000,]

```

Now that we have our data somewhat preprocessed, let's first make some observations about frequency and term use.

```{r frequency and use, warning = F, message = F}
# unnest tokens
cat_counts <- trimmed_r_cat_thread.df %>%
  unnest_tokens(output = word, input = comment, token = "words") %>%
  anti_join(stop_words) %>%
  count(word, sort = T)

dog_counts <- trimmed_r_dog_thread.df %>%
  unnest_tokens(output = word, input = comment, token = "words") %>%
  anti_join(stop_words) %>%
  count(word, sort = T)

# comparing terms shared and distinct between groups
comparison_all <-
  cat_counts %>% full_join(dog_counts, by = "word") %>%
  mutate(c_in = if_else(is.na(n.x), 0, 1),
  d_in = if_else(is.na(n.y), 0, 1))
with(comparison_all, CrossTable(c_in, d_in, prop.chisq = F))

```

This table tells us that there are 8091 word types in the dog thread, and 2305 (29%) of these types appear in the cat thread. Conversely, there are 3977 word types that appear in the cat thread, and 2305 (58%) of them appear in the dog thread.

What we've found can be summarized as follows: If a word is randomly selected in one of the dog thread comments, then there is a 30% chance that it will also appear in the cat thread comments.  This is also called the conditional probability that the word appears in the cat thread comments *given that* it appears in the dog thread comments.

What does such a finding *mean* though? One observation we can make is that the dog lovers seem to use more unique word types than the cat lovers. The raw count of unique word types alone, though, does not give us a solid point of comparison because the dog group may just use way more words in general.

To account for this, we could use a statistic called Type Token Ratio. To calculate this, we take the total number of token types and divide that by the total number of tokens used.

```{r type token ratio}

ttr_cat <- nrow(cat_counts)/sum(cat_counts$n)
ttr_dog <- nrow(dog_counts)/sum(dog_counts$n)

# what does our calculation reveal? What accounts for the results we see?

```

While we could do so much more with Type Token Ratio and other summary statistics, we want to conclude the instructional portion of our workshop by speaking about the process of training a classifier to use word types to answer our initial question: Do cat and dog people speak different langauges?

We'll begin by installing and loading some more libraries and then adjusting our data a little more for ease of comparison.

```{r loading and adjusting, message = F, warning = F}

# load tm package
#install.packages('tm')
library(tm)

# adjust data to add unique ids, combine, and shuffle
trimmed_r_cat_thread.df$u_id <- 1:nrow(trimmed_r_cat_thread.df)
trimmed_r_dog_thread.df$u_id <- (nrow(trimmed_r_dog_thread.df)+1):(2*nrow(trimmed_r_dog_thread.df))
cat_dog <- rbind(trimmed_r_cat_thread.df, trimmed_r_dog_thread.df)

set.seed(1234)
cat_dog <- cat_dog[sample(1:nrow(cat_dog)),]

# create class factor of subreddit
cat_dog$subreddit <- as.factor(cat_dog$subreddit)

```

The next step of the process will help us convert our text data into a document term matrix, which is a fancy way of describing an object with columns of word types and rows based on documents in our corpus (the comment objects). This information about presence of word types in each comment object will help our classifier make predictions about whether a comment comes from the cat set or the dog set.

Let's put our tm package to use to remove unnecessary characters:

```{r using tm, warning = F, message = F}

# Creating the document-term matrix for train data
doc.vec_train <- VectorSource(cat_dog$comment)
#doc.vec_train <- VectorSource(enc2utf8(cat_dog$comment))
doc.corpus_train <- Corpus(doc.vec_train)
doc.corpus_train <- tm_map(doc.corpus_train , tolower) # if this line throws an error,
# uncomment line 489 and rerun this section from there.
doc.corpus_train <- tm_map(doc.corpus_train, removePunctuation)
doc.corpus_train <- tm_map(doc.corpus_train, removeNumbers)
doc.corpus_train <- tm_map(doc.corpus_train, removeWords, stopwords("english"))
doc.corpus_train <- tm_map(doc.corpus_train, stripWhitespace)

DTM_all <- DocumentTermMatrix(doc.corpus_train)

# removing sparse terms
DTM_all <- removeSparseTerms(DTM_all, .999)

# convert train data to df
df_dtm <- as.data.frame(as.matrix(DTM_all))

# add dependent variable
df_dtm$Subreddit <- cat_dog$subreddit

```

Okay! Now that we have a document term matrix and our dependent variables in one place, let's go ahead train a classifier and get some performance results! We'll use the caret package to do some k-fold cross validation, and we'll use a Naive Bayes classifier.

According to Jason Brownlee, k-fold cross validation generally follows this procedure:

1. Shuffle the dataset randomly.
2. Split the dataset into k groups
3. For each unique group:
  1. Take the group as a hold out or test data set
  2. Take the remaining groups as a training data set
  3. Fit a model on the training set and evaluate it on the test set
  4. Retain the evaluation score and discard the model
4. Summarize the skill of the model using the sample of model evaluation scores

Using R's caret package, we will create a list of k folds and measure how our model does using each fold to test it.

```{r train and run, warning = F, message = F, results = 'hide'}

# load caret package
#install.packages('caret')
#install.packages('e1071')
library(caret)
library(e1071)

# create 10 folds for model validation
folds <- createFolds(df_dtm$subreddit, k = 10)

# train classifier on each fold
word_bayes_cv <- lapply(folds, function(x){
  training_fold <- df_dtm[-x, ]
  test_fold <- df_dtm[x, ]
  classifier <- naiveBayes(x = training_fold[, -ncol(df_dtm)],
                           y = training_fold$Subreddit)
  y_pred <- predict(classifier, newdata = test_fold[-ncol(df_dtm)])
  cm <- table(test_fold[, ncol(df_dtm)], y_pred)
  accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
  return(accuracy)
})

# observing results
acc_df <- as_tibble(do.call(rbind, word_bayes_cv))
acc_df
mean(acc_df$V1)

```

## Talking about results and concluding

In our run at this set, we can see that, with word features alone, our Naive Bayes classifier does not achieve a high accuracy rate for separating dog lovers from our Reddit dataset from cat lovers. What we might be able to conclude, then, is that these two groups, unlike Mr. Argument-Starter suggested, speak a similar language...or at least use similar words. As we noticed earlier, 60% of the token types appearing in the cat comments also appeared in the dog comments. While the dog commenters used more word types (and more words in general) than the cat commenters, perhaps we could check to see if many of these words appeared sparsely.

So what's next? We could reconsider the types of characters we remove from our word features and perhaps adjust some of our classifier's default parameters.

We could also shift our analytical focus from observations about types of words used to observations *around* language. Our original Reddit data set comes with a lot of metadata (score, up_vote_prop, controversiality). We could also apply a type token ratio metric to each individual comment in our set to see if a trend exists among cat lovers or dog lovers. We could also do some work with sentiment analysis to discover if cat and dog lovers evoke similar feelings when discussing their animals of choice. As a side note, it would be fun to see how cat lovers talk about dogs and vice versa.







